{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Security Findings Analysis Pipeline\n",
    "\n",
    "This notebook implements an interactive pipeline for grouping security findings and suggesting aggregated solutions using machine learning techniques."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load sample data (replace this with your actual data loading)\n",
    "fake = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'description_for_embedding': [\n",
    "        f\"{'JS' if i % 3 == 0 else 'Python' if i % 3 == 1 else 'Java'} security issue: {['XSS vulnerability', 'SQL injection', 'Outdated package', 'Insecure authentication'][i % 4]}\"\n",
    "        for i in range(100)\n",
    "    ],\n",
    "    'severity': np.random.choice(['Low', 'Medium', 'High'], 100)\n",
    "})\n",
    "\n",
    "findings = pd.read_json(Path(\"../../data/VulnerabilityReport_all_llama3_instruct.json\"))\n",
    "# add id and description_str\n",
    "findings['id'] = range(1, len(findings) + 1)\n",
    "findings['description_str'] = findings['description'].apply(lambda x: \"\".join([s for s in x]).replace(\"\\n\",\" \").strip())\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def process_search_terms(search_terms): # Frag nicht wie kompliziert\n",
    "    items = [x for x in re.split(r'\\s*-\\s*', search_terms)]\n",
    "    placeholder = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "    cleaned_items = [re.sub(r'[;\\n]' , ' ', item ).replace(' ', placeholder).replace(\" \", \"-\").replace(\"--\", \" \").replace(placeholder, \" \").strip() for item in items]\n",
    "    return re.sub(r'\\s+', ' ', ' '.join(cleaned_items))\n",
    "\n",
    "findings['description_for_embedding'] = findings['solution'].apply(lambda x: process_search_terms(x['search_terms']))\n",
    "\n",
    "\n",
    "display(findings.head())\n",
    "print(f\"\\nTotal findings: {len(findings)}\")\n",
    "\n",
    "findings.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Text Embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(findings['description_for_embedding'].tolist())\n",
    "\n",
    "print(f\"\\nShape of embeddings: {embeddings.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "max_clusters = len(findings) // 4\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(embeddings, kmeans.labels_))\n",
    "\n",
    "# print(f\"\\nInertias: {inertias}\")\n",
    "# print(f\"Silhouette Scores: {silhouette_scores}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Elbow method\n",
    "kl = KneeLocator(range(2, max_clusters + 1), inertias, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_clusters_elbow = kl.elbow\n",
    "\n",
    "print(f\"\\nOptimal number of clusters (Elbow method): {optimal_clusters_elbow}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Silhouette analysis\n",
    "optimal_clusters_silhouette = max(range(2, max_clusters + 1), key=lambda k: silhouette_scores[k - 2])\n",
    "\n",
    "print(f\"Optimal number of clusters (Silhouette analysis): {optimal_clusters_silhouette}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KMEANS"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose the optimal number of clusters based on your preference\n",
    "if optimal_clusters_elbow is not None:\n",
    "    optimal_clusters = optimal_clusters_elbow\n",
    "else:\n",
    "    optimal_clusters = optimal_clusters_silhouette\n",
    "\n",
    "print(f\"\\nChosen optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# histogram of cluster sizes\n",
    "df_cluster_sizes = pd.DataFrame({\n",
    "    'Cluster': range(optimal_clusters),\n",
    "    'Size': np.bincount(cluster_labels)\n",
    "})\n",
    "\n",
    "df_cluster_sizes = df_cluster_sizes[df_cluster_sizes['Size'] > 0]\n",
    "\n",
    "df_cluster_sizes.plot(x='Cluster', y='Size', kind='bar', title='Cluster Sizes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agglomerative Clustering"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def find_max_value ():\n",
    "    tmp = []\n",
    "    for i in range(0, 10):\n",
    "        granularity_factor = i / 10\n",
    "        tmp.append(granularity_factor * len(findings))\n",
    "    return max(tmp)\n",
    "\n",
    "# 1. Normalize the embeddings\n",
    "scaler = StandardScaler()\n",
    "normalized_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# 2. Apply DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=optimal_clusters, distance_threshold=None)\n",
    "clusters = clustering.fit_predict(normalized_embeddings)\n",
    "\n",
    "# 3. Get the results\n",
    "n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "n_noise = list(clusters).count(-1)\n",
    "\n",
    "# print(f\"\\nDistance threshold: {find_max_value()}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Perform dimensionality reduction using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create a DataFrame with the PCA-reduced embeddings, cluster labels, and severity\n",
    "df_plot = pd.DataFrame({\n",
    "    'PC1': embeddings_pca[:, 0],\n",
    "    'PC2': embeddings_pca[:, 1],\n",
    "    'PC3': embeddings_pca[:, 2],\n",
    "    'Cluster': cluster_labels,\n",
    "    'Severity': findings['severity'],\n",
    "    'Solution': findings['solution'].apply(lambda x: x['short_description'])\n",
    "})\n",
    "\n",
    "# Create an interactive 3D scatter plot\n",
    "fig = px.scatter_3d(df_plot, x='PC1', y='PC2', z='PC3', color='Cluster', symbol='Cluster', color_continuous_scale='Agsunset',\n",
    "                    hover_data={'Severity': True, 'Cluster': True, 'Solution':False},\n",
    "                    title='Interactive 3D Visualization of Clustered Findings')\n",
    "\n",
    "# Update the plot layout\n",
    "fig.update_layout(scene=dict(xaxis_title='PC1', yaxis_title='PC2', zaxis_title='PC3'))\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print max 5 descriptions of the first 2 clusters\n",
    "for i in range(4):\n",
    "    print(f\"\\nCluster {i + 1} ({len(findings[cluster_labels == i])} findings):\")\n",
    "    for desc in findings[cluster_labels == i]['description_for_embedding'][:5]:\n",
    "        print(f\"\\n- {desc}\")\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display average severity of each cluster as a bar chart\n",
    "df_severity = pd.DataFrame({\n",
    "    'Cluster': range(1, optimal_clusters+1),\n",
    "    'Average Severity': [findings[cluster_labels == i]['severity'].mean() for i in range(optimal_clusters)]\n",
    "})\n",
    "\n",
    "fig = px.bar(df_severity, x='Cluster', y='Average Severity', title='Average Severity of Each Cluster')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ToDo:\n",
    "- Get Cluster Names using LLM \n",
    "- Get Sub-Clusters using LLM\n",
    "- Get Aggregated Solutions for each Sub-Cluster / Cluster (tailored to roles)\n",
    "\n",
    "Alternative:\n",
    "- topic extraction\n",
    "- Cluster using topics\n",
    "- Get Solution for topics\n",
    "\n",
    "### Deliverables:\n",
    "- Static Analysis\n",
    "    - how many clusters are there\n",
    "    - which clusters have how many finding\n",
    "    - what is average/max severity\n",
    "- Recommendations for both Clusters and Subclusters, maybe even tailored to roles"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
